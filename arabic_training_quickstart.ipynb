{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f3ae66",
   "metadata": {},
   "source": [
    "# RepText Arabic Training - Quick Start Guide\n",
    "\n",
    "This notebook demonstrates how to pretrain RepText for Arabic text generation.\n",
    "\n",
    "## Prerequisites\n",
    "- NVIDIA GPU with at least 24GB VRAM\n",
    "- Python 3.8+\n",
    "- CUDA 11.7+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b310268",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c883b",
   "metadata": {},
   "source": [
    "## Step 2: Download Arabic Fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5deece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download recommended Arabic fonts from Google Fonts\n",
    "!python download_arabic_fonts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa043d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify fonts were downloaded\n",
    "import os\n",
    "fonts = [f for f in os.listdir('arabic_fonts') if f.endswith(('.ttf', '.otf'))]\n",
    "print(f\"Found {len(fonts)} fonts:\")\n",
    "for font in fonts:\n",
    "    print(f\"  - {font}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d34ac2",
   "metadata": {},
   "source": [
    "## Step 3: Test Font Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test Arabic text rendering\n",
    "test_text = \"ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸÉŸÖ ŸÅŸä RepText\"\n",
    "font_path = \"./arabic_fonts/Amiri-Regular.ttf\"\n",
    "\n",
    "img = Image.new('RGB', (600, 150), color='white')\n",
    "draw = ImageDraw.Draw(img)\n",
    "font = ImageFont.truetype(font_path, 60)\n",
    "draw.text((50, 40), test_text, font=font, fill='black')\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Arabic Text Rendering Test')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Arabic text rendering works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c8234",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Dataset\n",
    "\n",
    "This will generate synthetic training samples with:\n",
    "- Glyph images (rendered Arabic text)\n",
    "- Position maps (location heatmaps)\n",
    "- Binary masks (text regions)\n",
    "- Canny edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For quick testing, use a small number of samples\n",
    "# For actual training, use 10000-50000\n",
    "NUM_SAMPLES = 100  # Change to 10000 for real training\n",
    "\n",
    "!python prepare_arabic_dataset.py \\\n",
    "    --output_dir ./arabic_training_data \\\n",
    "    --font_dir ./arabic_fonts \\\n",
    "    --text_file ./arabic_texts.txt \\\n",
    "    --num_samples {NUM_SAMPLES} \\\n",
    "    --width 1024 \\\n",
    "    --height 1024 \\\n",
    "    --min_font_size 60 \\\n",
    "    --max_font_size 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c90d2",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a sample\n",
    "sample_dir = \"./arabic_training_data/sample_000000\"\n",
    "\n",
    "# Load images\n",
    "glyph = Image.open(f\"{sample_dir}/glyph.png\")\n",
    "position = Image.open(f\"{sample_dir}/position.png\")\n",
    "mask = Image.open(f\"{sample_dir}/mask.png\")\n",
    "canny = Image.open(f\"{sample_dir}/canny.png\")\n",
    "\n",
    "# Load metadata\n",
    "with open(f\"{sample_dir}/metadata.json\", 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "axes[0, 0].imshow(glyph)\n",
    "axes[0, 0].set_title(f\"Glyph: {metadata['text']}\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(position)\n",
    "axes[0, 1].set_title(\"Position Map\")\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(mask, cmap='gray')\n",
    "axes[1, 0].set_title(\"Text Mask\")\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(canny)\n",
    "axes[1, 1].set_title(\"Canny Edges\")\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Training Sample - Font Size: {metadata['font_size']}px\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f26e8c",
   "metadata": {},
   "source": [
    "## Step 6: Test Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdd7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabic_dataset import create_dataloaders\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    data_dir='./arabic_training_data',\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    image_size=(1024, 1024),\n",
    "    train_ratio=0.9\n",
    ")\n",
    "\n",
    "# Test loading a batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch contents:\")\n",
    "print(f\"  Glyph shape: {batch['glyph'].shape}\")\n",
    "print(f\"  Position shape: {batch['position'].shape}\")\n",
    "print(f\"  Mask shape: {batch['mask'].shape}\")\n",
    "print(f\"  Canny shape: {batch['canny'].shape}\")\n",
    "print(f\"  Text samples: {batch['text']}\")\n",
    "print(f\"  Font sizes: {batch['font_size']}\")\n",
    "print(\"\\n‚úì Dataset loading works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde47a3d",
   "metadata": {},
   "source": [
    "## Step 7: Configure Accelerate for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f74d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d148693",
   "metadata": {},
   "source": [
    "## Step 8: Review Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('train_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb1d01",
   "metadata": {},
   "source": [
    "## Step 9: Launch Training\n",
    "\n",
    "**Note:** Training takes a long time. You may want to run this in a terminal instead of notebook.\n",
    "\n",
    "For terminal:\n",
    "```bash\n",
    "accelerate launch train_arabic.py --config train_config.yaml\n",
    "```\n",
    "\n",
    "Or use the automated script:\n",
    "```bash\n",
    "./train_arabic.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For notebook training (not recommended for long training runs)\n",
    "# Uncomment to run:\n",
    "# !accelerate launch train_arabic.py --config train_config.yaml\n",
    "\n",
    "print(\"It's recommended to run training in a terminal or tmux session:\")\n",
    "print(\"  accelerate launch train_arabic.py --config train_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675dd05",
   "metadata": {},
   "source": [
    "## Step 9.5: Monitor Training Progress (RunPod)\n",
    "\n",
    "‚è≥ **Training Status: IN PROGRESS (Epoch 2/100)**\n",
    "\n",
    "Your training started successfully! You can see it processing batches per epoch in the terminal.\n",
    "\n",
    "**Understanding the Progress:**\n",
    "- Progress shows: `23/90 [00:33<01:37...]` = 23 samples processed out of 90 training samples\n",
    "- This means each epoch processes the full 90 training samples\n",
    "- After all 90 batches complete, the next epoch begins\n",
    "- **Total:** 100 epochs to complete (configured in `train_config.yaml`)\n",
    "- **Time per epoch:** ~1.5 minutes\n",
    "- **Total training time:** ~2.5 hours\n",
    "\n",
    "**Key Metrics to Watch:**\n",
    "```\n",
    "Epoch 2: 26% | 23/90 [00:33<01:37, 1.45s/it, diffusion_loss=0, loss=0, lr=4e-8]\n",
    " ‚Üì       ‚Üì    ‚Üì  ‚Üì                                    ‚Üì\n",
    "Epoch   Progress Batch count      Time metrics        Learning rate\n",
    "        of epoch\n",
    "```\n",
    "\n",
    "**Monitor Training:**\n",
    "\n",
    "Run in your RunPod terminal:\n",
    "```bash\n",
    "# Check if training is still running\n",
    "ps aux | grep train_arabic.py\n",
    "\n",
    "# Check checkpoint directory growth (as training progresses)\n",
    "watch -n 30 \"ls -lh output/arabic_reptext/ && echo '---' && du -sh output/arabic_reptext/\"\n",
    "\n",
    "# Check if intermediate checkpoints are being saved\n",
    "ls -lh output/arabic_reptext/checkpoint-*/\n",
    "```\n",
    "\n",
    "**When Training Completes:**\n",
    "- Final checkpoint saved to: `output/arabic_reptext/final_model/`\n",
    "- Should contain:\n",
    "  - `config.json` (with `in_channels: 64`)\n",
    "  - `diffusion_pytorch_model.safetensors` (~5GB)\n",
    "- This checkpoint will be **100% compatible** with inference ‚úÖ\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da5d32",
   "metadata": {},
   "source": [
    "## Step 10: Monitor Training (Optional - with W&B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wandb\n",
    "# !pip install wandb\n",
    "# !wandb login\n",
    "\n",
    "# Then run with --use_wandb flag:\n",
    "# !accelerate launch train_arabic.py --config train_config.yaml --use_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbe107",
   "metadata": {},
   "source": [
    "## Step 11: Test Your Trained Model\n",
    "\n",
    "After training completes, test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Collect all progress images\n",
    "progress_dir = \"./training_progress\"\n",
    "image_files = []\n",
    "\n",
    "# Add baseline\n",
    "if os.path.exists(f\"{progress_dir}/baseline_original.jpg\"):\n",
    "    image_files.append((\"Baseline\\n(Original RepText)\", f\"{progress_dir}/baseline_original.jpg\"))\n",
    "\n",
    "# Add epoch images (every 2 epochs: 2, 4, 6, 8, 10)\n",
    "for epoch in range(2, 11, 2):\n",
    "    img_path = f\"{progress_dir}/epoch_{epoch}.jpg\"\n",
    "    if os.path.exists(img_path):\n",
    "        image_files.append((f\"Epoch {epoch}\", img_path))\n",
    "\n",
    "# Create comparison grid\n",
    "if len(image_files) > 0:\n",
    "    n_images = len(image_files)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_images + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
    "    axes = axes.flatten() if n_images > 1 else [axes]\n",
    "    \n",
    "    for idx, (title, img_path) in enumerate(image_files):\n",
    "        img = Image.open(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(title, fontsize=14, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_images, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    test_text = \"ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ Ÿàÿ±ÿ≠ŸÖÿ© ÿßŸÑŸÑŸá Ÿàÿ®ÿ±ŸÉÿßÿ™Ÿá\"\n",
    "    plt.suptitle(f'Training Progress: {test_text}', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./training_progress/comparison_grid.jpg', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Compared {len(image_files)} images\")\n",
    "    print(f\"üìä Grid saved: ./training_progress/comparison_grid.jpg\")\n",
    "else:\n",
    "    print(\"‚ùå No images found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7219d9e9",
   "metadata": {},
   "source": [
    "## Step 12: Train with Progress Monitoring\n",
    "\n",
    "This cell will:\n",
    "\n",
    "1. Generate baseline image with original RepText3. Save final model\n",
    "2. Train for 10 epochs (generating test image every 2 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import subprocess\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, clear_output\n",
    "from controlnet_flux import FluxControlNetModel\n",
    "from pipeline_flux_controlnet import FluxControlNetPipeline\n",
    "\n",
    "# Configuration\n",
    "test_text = \"ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ Ÿàÿ±ÿ≠ŸÖÿ© ÿßŸÑŸÑŸá Ÿàÿ®ÿ±ŸÉÿßÿ™Ÿá\"\n",
    "test_prompt = \"a street sign in city\"\n",
    "total_epochs = 10\n",
    "epochs_per_step = 2  # Generate image every 2 epochs\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REPTEXT ARABIC FINE-TUNING WITH PROGRESS MONITORING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test text: {test_text}\")\n",
    "print(f\"Total epochs: {total_epochs}\")\n",
    "print(f\"Image generation interval: every {epochs_per_step} epochs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Helper functions\n",
    "def canny(img):\n",
    "    low_threshold = 50\n",
    "    high_threshold = 100\n",
    "    img = cv2.Canny(img, low_threshold, high_threshold)\n",
    "    img = img[:, :, None]\n",
    "    img = 255 - np.concatenate([img, img, img], axis=2)\n",
    "    return img\n",
    "\n",
    "def generate_test_image(controlnet_path, epoch_label, text=test_text, prompt=test_prompt):\n",
    "    \"\"\"Generate test image with current model\"\"\"\n",
    "    print(f\"\\nüì∏ Generating image for {epoch_label}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load models\n",
    "        base_model = \"black-forest-labs/FLUX.1-dev\"\n",
    "        controlnet = FluxControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.bfloat16)\n",
    "        pipe = FluxControlNetPipeline.from_pretrained(\n",
    "            base_model, controlnet=controlnet, torch_dtype=torch.bfloat16\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Setup\n",
    "        width, height = 1024, 512\n",
    "        font_path = \"./arabic_fonts/Amiri-Regular.ttf\"\n",
    "        font_size = 80\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "        \n",
    "        # Prepare control images\n",
    "        text_position = (200, 200)\n",
    "        text_color = (255, 255, 255)\n",
    "        \n",
    "        control_image_glyph = Image.new(\"RGB\", (width, height), (0, 0, 0))\n",
    "        draw = ImageDraw.Draw(control_image_glyph)\n",
    "        draw.text(text_position, text, font=font, fill=text_color)\n",
    "        bbox = draw.textbbox(text_position, text, font=font)\n",
    "        \n",
    "        # Position map\n",
    "        control_position = np.zeros([height, width], dtype=np.uint8)\n",
    "        control_position[bbox[1]:bbox[3], bbox[0]:bbox[2]] = 255\n",
    "        control_position = Image.fromarray(control_position)\n",
    "        \n",
    "        # Regional mask\n",
    "        control_mask_np = np.zeros([height, width], dtype=np.uint8)\n",
    "        control_mask_np[bbox[1]-5:bbox[3]+5, bbox[0]-5:bbox[2]+5] = 255\n",
    "        control_mask = Image.fromarray(control_mask_np)\n",
    "        \n",
    "        # Glyph\n",
    "        control_glyph = np.array(control_image_glyph)\n",
    "        control_glyph = Image.fromarray(control_glyph).convert(\"RGB\")\n",
    "        \n",
    "        # Canny\n",
    "        control_image = canny(cv2.cvtColor(np.array(control_image_glyph), cv2.COLOR_RGB2BGR))\n",
    "        control_image = Image.fromarray(cv2.cvtColor(control_image, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Generate\n",
    "        full_prompt = f\"{prompt}, '{text}', filmfotos, film grain, reversal film photography\"\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "        \n",
    "        image = pipe(\n",
    "            full_prompt,\n",
    "            control_image=[control_image],\n",
    "            control_position=[control_position],\n",
    "            control_mask=[control_mask],\n",
    "            control_glyph=control_glyph,\n",
    "            controlnet_conditioning_scale=1.0,\n",
    "            controlnet_conditioning_step=30,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=3.5,\n",
    "            generator=generator,\n",
    "        ).images[0]\n",
    "        \n",
    "        # Save\n",
    "        os.makedirs(\"./training_progress\", exist_ok=True)\n",
    "        output_path = f\"./training_progress/{epoch_label}.jpg\"\n",
    "        image.save(output_path)\n",
    "        \n",
    "        print(f\"   Saved: {output_path}\")\n",
    "        display(image)\n",
    "        \n",
    "        # Cleanup\n",
    "        del pipe, controlnet\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error generating image: {e}\")\n",
    "        return None\n",
    "\n",
    "# STEP 1: Generate baseline with original RepText\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: BASELINE - Original RepText Model\")\n",
    "print(\"=\"*80)\n",
    "generate_test_image(\"Shakker-Labs/RepText\", \"baseline_original\")\n",
    "\n",
    "# STEP 2: Update config for training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: PREPARING TRAINING CONFIG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with open(\"train_config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['model']['pretrained_controlnet'] = \"Shakker-Labs/RepText\"\n",
    "original_epochs = config['training']['num_epochs']\n",
    "config['training']['num_epochs'] = epochs_per_step\n",
    "\n",
    "with open(\"train_config.yaml\", 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"‚úÖ Config updated: {epochs_per_step} epochs per training step\")\n",
    "\n",
    "# STEP 3: Training loop with progress monitoring\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"STEP 3: TRAINING FOR {total_epochs} EPOCHS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "epoch_count = 0\n",
    "while epoch_count < total_epochs:\n",
    "    current_step = epoch_count // epochs_per_step + 1\n",
    "    epochs_this_step = min(epochs_per_step, total_epochs - epoch_count)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING STEP {current_step}: Epochs {epoch_count+1}-{epoch_count+epochs_this_step}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Update config for this step\n",
    "    with open(\"train_config.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    config['training']['num_epochs'] = epochs_this_step\n",
    "    \n",
    "    # After first step, load from checkpoint instead of pretrained\n",
    "    if epoch_count > 0:\n",
    "        checkpoint_path = os.path.join(config['output']['output_dir'], \"checkpoint\")\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            config['model']['pretrained_controlnet'] = checkpoint_path\n",
    "    \n",
    "    with open(\"train_config.yaml\", 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "    \n",
    "    # Run training\n",
    "    print(f\"\\nüöÄ Launching training...\")\n",
    "    result = subprocess.run(\n",
    "        [\"accelerate\", \"launch\", \"train_arabic.py\", \"--config\", \"train_config.yaml\"],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"\\n‚ùå Training failed at step {current_step}\")\n",
    "        break\n",
    "    \n",
    "    epoch_count += epochs_this_step\n",
    "    \n",
    "    # Save checkpoint for next iteration\n",
    "    checkpoint_src = os.path.join(config['output']['output_dir'], \"final_model\")\n",
    "    checkpoint_dst = os.path.join(config['output']['output_dir'], \"checkpoint\")\n",
    "    \n",
    "    if os.path.exists(checkpoint_src):\n",
    "        import shutil\n",
    "        if os.path.exists(checkpoint_dst):\n",
    "            shutil.rmtree(checkpoint_dst)\n",
    "        shutil.copytree(checkpoint_src, checkpoint_dst)\n",
    "    \n",
    "    # Generate test image\n",
    "    if os.path.exists(checkpoint_dst):\n",
    "        generate_test_image(checkpoint_dst, f\"epoch_{epoch_count}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed {epoch_count}/{total_epochs} epochs\")\n",
    "\n",
    "# STEP 4: Training complete\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìÅ Results:\")\n",
    "print(f\"   Baseline: ./training_progress/baseline_original.jpg\")\n",
    "for i in range(epochs_per_step, total_epochs + 1, epochs_per_step):\n",
    "    print(f\"   Epoch {i}: ./training_progress/epoch_{i}.jpg\")\n",
    "print(f\"\\nüì¶ Final model: {config['output']['output_dir']}/final_model/\")\n",
    "\n",
    "# Restore original config\n",
    "with open(\"train_config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config['training']['num_epochs'] = original_epochs\n",
    "config['model']['pretrained_controlnet'] = \"Shakker-Labs/RepText\"\n",
    "with open(\"train_config.yaml\", 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4c632",
   "metadata": {},
   "source": [
    "## Step 13: Visualize All Progress Images\n",
    "\n",
    "Compare baseline vs all training checkpoints."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
