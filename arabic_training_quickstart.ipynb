{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f3ae66",
   "metadata": {},
   "source": [
    "# RepText Arabic Training - Quick Start Guide\n",
    "\n",
    "This notebook demonstrates how to pretrain RepText for Arabic text generation.\n",
    "\n",
    "## Prerequisites\n",
    "- NVIDIA GPU with at least 24GB VRAM\n",
    "- Python 3.8+\n",
    "- CUDA 11.7+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b310268",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c883b",
   "metadata": {},
   "source": [
    "## Step 2: Download Arabic Fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5deece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download recommended Arabic fonts from Google Fonts\n",
    "!python download_arabic_fonts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa043d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify fonts were downloaded\n",
    "import os\n",
    "fonts = [f for f in os.listdir('arabic_fonts') if f.endswith(('.ttf', '.otf'))]\n",
    "print(f\"Found {len(fonts)} fonts:\")\n",
    "for font in fonts:\n",
    "    print(f\"  - {font}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d34ac2",
   "metadata": {},
   "source": [
    "## Step 3: Test Font Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test Arabic text rendering\n",
    "test_text = \"ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸÉŸÖ ŸÅŸä RepText\"\n",
    "font_path = \"./arabic_fonts/Amiri-Regular.ttf\"\n",
    "\n",
    "img = Image.new('RGB', (600, 150), color='white')\n",
    "draw = ImageDraw.Draw(img)\n",
    "font = ImageFont.truetype(font_path, 60)\n",
    "draw.text((50, 40), test_text, font=font, fill='black')\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Arabic Text Rendering Test')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Arabic text rendering works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c8234",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Dataset\n",
    "\n",
    "This will generate synthetic training samples with:\n",
    "- Glyph images (rendered Arabic text)\n",
    "- Position maps (location heatmaps)\n",
    "- Binary masks (text regions)\n",
    "- Canny edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For quick testing, use a small number of samples\n",
    "# For actual training, use 10000-50000\n",
    "NUM_SAMPLES = 100  # Change to 10000 for real training\n",
    "\n",
    "!python prepare_arabic_dataset.py \\\n",
    "    --output_dir ./arabic_training_data \\\n",
    "    --font_dir ./arabic_fonts \\\n",
    "    --text_file ./arabic_texts.txt \\\n",
    "    --num_samples {NUM_SAMPLES} \\\n",
    "    --width 1024 \\\n",
    "    --height 1024 \\\n",
    "    --min_font_size 60 \\\n",
    "    --max_font_size 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c90d2",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a sample\n",
    "sample_dir = \"./arabic_training_data/sample_000000\"\n",
    "\n",
    "# Load images\n",
    "glyph = Image.open(f\"{sample_dir}/glyph.png\")\n",
    "position = Image.open(f\"{sample_dir}/position.png\")\n",
    "mask = Image.open(f\"{sample_dir}/mask.png\")\n",
    "canny = Image.open(f\"{sample_dir}/canny.png\")\n",
    "\n",
    "# Load metadata\n",
    "with open(f\"{sample_dir}/metadata.json\", 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "axes[0, 0].imshow(glyph)\n",
    "axes[0, 0].set_title(f\"Glyph: {metadata['text']}\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(position)\n",
    "axes[0, 1].set_title(\"Position Map\")\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(mask, cmap='gray')\n",
    "axes[1, 0].set_title(\"Text Mask\")\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(canny)\n",
    "axes[1, 1].set_title(\"Canny Edges\")\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Training Sample - Font Size: {metadata['font_size']}px\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f26e8c",
   "metadata": {},
   "source": [
    "## Step 6: Test Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdd7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabic_dataset import create_dataloaders\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    data_dir='./arabic_training_data',\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    image_size=(1024, 1024),\n",
    "    train_ratio=0.9\n",
    ")\n",
    "\n",
    "# Test loading a batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch contents:\")\n",
    "print(f\"  Glyph shape: {batch['glyph'].shape}\")\n",
    "print(f\"  Position shape: {batch['position'].shape}\")\n",
    "print(f\"  Mask shape: {batch['mask'].shape}\")\n",
    "print(f\"  Canny shape: {batch['canny'].shape}\")\n",
    "print(f\"  Text samples: {batch['text']}\")\n",
    "print(f\"  Font sizes: {batch['font_size']}\")\n",
    "print(\"\\n‚úì Dataset loading works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde47a3d",
   "metadata": {},
   "source": [
    "## Step 7: Configure Accelerate for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f74d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d148693",
   "metadata": {},
   "source": [
    "## Step 8: Review Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('train_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb1d01",
   "metadata": {},
   "source": [
    "## Step 9: Launch Training\n",
    "\n",
    "**Note:** Training takes a long time. You may want to run this in a terminal instead of notebook.\n",
    "\n",
    "For terminal:\n",
    "```bash\n",
    "accelerate launch train_arabic.py --config train_config.yaml\n",
    "```\n",
    "\n",
    "Or use the automated script:\n",
    "```bash\n",
    "./train_arabic.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For notebook training (not recommended for long training runs)\n",
    "# Uncomment to run:\n",
    "# !accelerate launch train_arabic.py --config train_config.yaml\n",
    "\n",
    "print(\"It's recommended to run training in a terminal or tmux session:\")\n",
    "print(\"  accelerate launch train_arabic.py --config train_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675dd05",
   "metadata": {},
   "source": [
    "## Step 9.5: Monitor Training Progress (RunPod)\n",
    "\n",
    "‚è≥ **Training Status: IN PROGRESS (Epoch 2/100)**\n",
    "\n",
    "Your training started successfully! You can see it processing batches per epoch in the terminal.\n",
    "\n",
    "**Understanding the Progress:**\n",
    "- Progress shows: `23/90 [00:33<01:37...]` = 23 samples processed out of 90 training samples\n",
    "- This means each epoch processes the full 90 training samples\n",
    "- After all 90 batches complete, the next epoch begins\n",
    "- **Total:** 100 epochs to complete (configured in `train_config.yaml`)\n",
    "- **Time per epoch:** ~1.5 minutes\n",
    "- **Total training time:** ~2.5 hours\n",
    "\n",
    "**Key Metrics to Watch:**\n",
    "```\n",
    "Epoch 2: 26% | 23/90 [00:33<01:37, 1.45s/it, diffusion_loss=0, loss=0, lr=4e-8]\n",
    " ‚Üì       ‚Üì    ‚Üì  ‚Üì                                    ‚Üì\n",
    "Epoch   Progress Batch count      Time metrics        Learning rate\n",
    "        of epoch\n",
    "```\n",
    "\n",
    "**Monitor Training:**\n",
    "\n",
    "Run in your RunPod terminal:\n",
    "```bash\n",
    "# Check if training is still running\n",
    "ps aux | grep train_arabic.py\n",
    "\n",
    "# Check checkpoint directory growth (as training progresses)\n",
    "watch -n 30 \"ls -lh output/arabic_reptext/ && echo '---' && du -sh output/arabic_reptext/\"\n",
    "\n",
    "# Check if intermediate checkpoints are being saved\n",
    "ls -lh output/arabic_reptext/checkpoint-*/\n",
    "```\n",
    "\n",
    "**When Training Completes:**\n",
    "- Final checkpoint saved to: `output/arabic_reptext/final_model/`\n",
    "- Should contain:\n",
    "  - `config.json` (with `in_channels: 64`)\n",
    "  - `diffusion_pytorch_model.safetensors` (~5GB)\n",
    "- This checkpoint will be **100% compatible** with inference ‚úÖ\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da5d32",
   "metadata": {},
   "source": [
    "## Step 10: Monitor Training (Optional - with W&B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wandb\n",
    "# !pip install wandb\n",
    "# !wandb login\n",
    "\n",
    "# Then run with --use_wandb flag:\n",
    "# !accelerate launch train_arabic.py --config train_config.yaml --use_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbe107",
   "metadata": {},
   "source": [
    "## Step 11: Test Your Trained Model\n",
    "\n",
    "After training completes, test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training and generate images after each epoch\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "test_text = \"ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ Ÿàÿ±ÿ≠ŸÖÿ© ÿßŸÑŸÑŸá Ÿàÿ®ÿ±ŸÉÿßÿ™Ÿá\"\n",
    "\n",
    "# Start training in background\n",
    "print(\"üöÄ Starting fine-tuning training...\")\n",
    "print(f\"üìä Test text for tracking: {test_text}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run one epoch at a time and generate images\n",
    "for epoch in range(1, 11):  # 10 epochs\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch}/10\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Update config to train for 1 epoch\n",
    "    with open(\"train_config.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    config['training']['num_epochs'] = 1\n",
    "    \n",
    "    # Set checkpoint to resume from if not first epoch\n",
    "    if epoch > 1:\n",
    "        config['model']['pretrained_controlnet'] = f\"./output/arabic_reptext/epoch_{epoch-1}\"\n",
    "    else:\n",
    "        config['model']['pretrained_controlnet'] = \"Shakker-Labs/RepText\"\n",
    "    \n",
    "    with open(\"train_config.yaml\", 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "    \n",
    "    # Run training for this epoch\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"accelerate\", \"launch\", \"train_arabic.py\", \"--config\", \"train_config.yaml\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=3600  # 1 hour timeout per epoch\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"‚ùå Training failed at epoch {epoch}\")\n",
    "            print(result.stderr)\n",
    "            break\n",
    "        \n",
    "        print(f\"‚úÖ Epoch {epoch} training complete\")\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"‚ö†Ô∏è Epoch {epoch} timed out\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during epoch {epoch}: {e}\")\n",
    "        break\n",
    "    \n",
    "    # Generate test image after this epoch\n",
    "    print(f\"\\nüì∏ Generating test image after epoch {epoch}...\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint_path = f\"./output/arabic_reptext/epoch_{epoch}\"\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            # Use final_model if epoch checkpoint doesn't exist\n",
    "            checkpoint_path = \"./output/arabic_reptext/final_model\"\n",
    "        \n",
    "        if os.path.exists(checkpoint_path):\n",
    "            generate_test_image(checkpoint_path, f\"epoch_{epoch}\", test_text)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Checkpoint not found: {checkpoint_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to generate image after epoch {epoch}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Small delay between epochs\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGenerated images saved in: ./training_progress/\")\n",
    "print(\"Model checkpoints saved in: ./output/arabic_reptext/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29220f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Collect all progress images\n",
    "progress_dir = \"./training_progress\"\n",
    "image_files = []\n",
    "\n",
    "# Add baseline\n",
    "if os.path.exists(f\"{progress_dir}/baseline_original.jpg\"):\n",
    "    image_files.append((\"Baseline\\n(Original RepText)\", f\"{progress_dir}/baseline_original.jpg\"))\n",
    "\n",
    "# Add all epochs\n",
    "for epoch in range(1, 11):\n",
    "    img_path = f\"{progress_dir}/epoch_{epoch}.jpg\"\n",
    "    if os.path.exists(img_path):\n",
    "        image_files.append((f\"Epoch {epoch}\", img_path))\n",
    "\n",
    "# Create grid visualization\n",
    "if len(image_files) > 0:\n",
    "    # Calculate grid size\n",
    "    n_images = len(image_files)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_images + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
    "    axes = axes.flatten() if n_images > 1 else [axes]\n",
    "    \n",
    "    for idx, (title, img_path) in enumerate(image_files):\n",
    "        img = Image.open(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(title, fontsize=14, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_images, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Training Progress: {test_text}', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./training_progress/comparison_grid.jpg', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Visualized {len(image_files)} images\")\n",
    "    print(f\"üìä Comparison grid saved to: ./training_progress/comparison_grid.jpg\")\n",
    "else:\n",
    "    print(\"‚ùå No progress images found. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a504a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. ‚úÖ **Baseline Generation**: Generated image with original RepText model\n",
    "2. ‚úÖ **Fine-Tuning**: Trained for 10 epochs on Arabic dataset\n",
    "3. ‚úÖ **Progress Tracking**: Generated test image after each epoch\n",
    "4. ‚úÖ **Comparison**: Visualized improvement across training\n",
    "\n",
    "### Key Results\n",
    "\n",
    "- **Test Text**: `ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ Ÿàÿ±ÿ≠ŸÖÿ© ÿßŸÑŸÑŸá Ÿàÿ®ÿ±ŸÉÿßÿ™Ÿá`\n",
    "- **Prompt**: `a street sign in city`\n",
    "- **Total Epochs**: 10\n",
    "- **Model Checkpoints**: Saved in `./output/arabic_reptext/epoch_X/`\n",
    "- **Progress Images**: Saved in `./training_progress/`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Compare baseline vs final epoch to see improvement\n",
    "2. Use the best checkpoint for your Arabic text generation tasks\n",
    "3. Experiment with different prompts and text\n",
    "4. Fine-tune for more epochs if needed\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `./output/arabic_reptext/epoch_1/` through `epoch_10/` - Model checkpoints\n",
    "- `./output/arabic_reptext/final_model/` - Final trained model\n",
    "- `./training_progress/baseline_original.jpg` - Original RepText output\n",
    "- `./training_progress/epoch_X.jpg` - Output after each epoch\n",
    "- `./training_progress/comparison_grid.jpg` - Side-by-side comparison\n",
    "\n",
    "You can now use the fine-tuned model for Arabic text generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f10f17",
   "metadata": {},
   "source": [
    "## Step 13: Visualize Training Progress\n",
    "\n",
    "Compare all generated images across epochs to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom training script that generates images after each epoch\n",
    "training_script_content = '''\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from diffusers import DDPMScheduler, AutoencoderKL\n",
    "from diffusers.optimization import get_scheduler\n",
    "from controlnet_flux import FluxControlNetModel\n",
    "from arabic_dataset import create_dataloaders\n",
    "from train_arabic import train_one_epoch\n",
    "import sys\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def main():\n",
    "    # Load config\n",
    "    with open(\"train_config.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Setup accelerator\n",
    "    accelerator_project_config = ProjectConfiguration(\n",
    "        project_dir=config['output']['output_dir'],\n",
    "        logging_dir=config['output']['logging_dir']\n",
    "    )\n",
    "    \n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "        mixed_precision=config['training']['mixed_precision'],\n",
    "        project_config=accelerator_project_config\n",
    "    )\n",
    "    \n",
    "    set_seed(42)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    os.makedirs(config['output']['output_dir'], exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"Loading base model: {config['model']['base_model']}\")\n",
    "    \n",
    "    # Load VAE\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        config['model']['base_model'],\n",
    "        subfolder=\"vae\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    vae.requires_grad_(False)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load pretrained ControlNet\n",
    "    pretrained_controlnet = config['model'].get('pretrained_controlnet')\n",
    "    logger.info(f\"Loading pretrained ControlNet: {pretrained_controlnet}\")\n",
    "    controlnet = FluxControlNetModel.from_pretrained(\n",
    "        pretrained_controlnet,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        controlnet.enable_gradient_checkpointing()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not enable gradient checkpointing: {e}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize noise scheduler\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "        config['model']['base_model'],\n",
    "        subfolder=\"scheduler\"\n",
    "    )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create dataloaders\n",
    "    logger.info(\"Creating dataloaders...\")\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        data_dir=config['data']['data_dir'],\n",
    "        batch_size=config['data']['batch_size'],\n",
    "        num_workers=config['data']['num_workers'],\n",
    "        image_size=tuple(config['data']['image_size']),\n",
    "        train_ratio=config['data']['train_ratio']\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        controlnet.parameters(),\n",
    "        lr=config['training']['learning_rate'],\n",
    "        betas=(config['training']['adam_beta1'], config['training']['adam_beta2']),\n",
    "        weight_decay=config['training']['adam_weight_decay'],\n",
    "        eps=config['training']['adam_epsilon']\n",
    "    )\n",
    "    \n",
    "    # Initialize LR scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        config['training']['lr_scheduler'],\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=config['training']['lr_warmup_steps'],\n",
    "        num_training_steps=config['training']['num_epochs'] * len(train_loader)\n",
    "    )\n",
    "    \n",
    "    # Prepare with accelerator\n",
    "    controlnet, optimizer, train_loader, val_loader, lr_scheduler = accelerator.prepare(\n",
    "        controlnet, optimizer, train_loader, val_loader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    vae.to(accelerator.device)\n",
    "    \n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_loader.dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {config['training']['num_epochs']}\")\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    # Training loop with epoch-wise image generation\n",
    "    for epoch in range(config['training']['num_epochs']):\n",
    "        global_step = train_one_epoch(\n",
    "            accelerator=accelerator,\n",
    "            controlnet=controlnet,\n",
    "            vae=vae,\n",
    "            noise_scheduler=noise_scheduler,\n",
    "            optimizer=optimizer,\n",
    "            train_loader=train_loader,\n",
    "            text_perceptual_loss=None,\n",
    "            config=config,\n",
    "            epoch=epoch,\n",
    "            global_step=global_step\n",
    "        )\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        if accelerator.is_main_process:\n",
    "            save_path = os.path.join(config['output']['output_dir'], f\"epoch_{epoch+1}\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            \n",
    "            unwrapped_controlnet = accelerator.unwrap_model(controlnet)\n",
    "            unwrapped_controlnet.save_pretrained(save_path)\n",
    "            \n",
    "            logger.info(f\"Saved epoch {epoch+1} checkpoint to {save_path}\")\n",
    "            \n",
    "            # Generate test image after this epoch\n",
    "            print(f\"\\\\n{'='*60}\")\n",
    "            print(f\"Generating test image after epoch {epoch+1}...\")\n",
    "            print(f\"{'='*60}\\\\n\")\n",
    "        \n",
    "        # Synchronize all processes\n",
    "        accelerator.wait_for_everyone()\n",
    "    \n",
    "    # Save final model\n",
    "    if accelerator.is_main_process:\n",
    "        save_path = os.path.join(config['output']['output_dir'], \"final_model\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        unwrapped_controlnet = accelerator.unwrap_model(controlnet)\n",
    "        unwrapped_controlnet.save_pretrained(save_path)\n",
    "        \n",
    "        logger.info(f\"Training complete! Model saved to {save_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save custom training script\n",
    "with open(\"train_arabic_with_tracking.py\", \"w\") as f:\n",
    "    f.write(training_script_content)\n",
    "\n",
    "print(\"‚úÖ Created custom training script with epoch tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2cc3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Update config for 10 epochs with epoch-wise checkpointing\n",
    "config_path = \"./train_config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set training parameters\n",
    "config['training']['num_epochs'] = 10\n",
    "config['training']['save_steps'] = 1000  # Save during epochs\n",
    "config['model']['pretrained_controlnet'] = \"Shakker-Labs/RepText\"\n",
    "\n",
    "# Save updated config\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(\"‚úÖ Config updated for 10-epoch training with RepText fine-tuning\")\n",
    "print(f\"   - Base model: {config['model']['pretrained_controlnet']}\")\n",
    "print(f\"   - Epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"   - Output: {config['output']['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9a203",
   "metadata": {},
   "source": [
    "## Step 12: Fine-Tune RepText on Arabic Dataset\n",
    "\n",
    "This will:\n",
    "1. Load the pretrained RepText model\n",
    "2. Fine-tune for 10 epochs on your Arabic dataset\n",
    "3. Generate a test image after each epoch to track progress\n",
    "4. Save checkpoints after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from controlnet_flux import FluxControlNetModel\n",
    "from pipeline_flux_controlnet import FluxControlNetPipeline\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "def canny(img):\n",
    "    low_threshold = 50\n",
    "    high_threshold = 100\n",
    "    img = cv2.Canny(img, low_threshold, high_threshold)\n",
    "    img = img[:, :, None]\n",
    "    img = 255 - np.concatenate([img, img, img], axis=2)\n",
    "    return img\n",
    "\n",
    "def generate_test_image(controlnet_model_path, epoch_name=\"baseline\", test_text=\"ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ Ÿàÿ±ÿ≠ŸÖÿ© ÿßŸÑŸÑŸá Ÿàÿ®ÿ±ŸÉÿßÿ™Ÿá\"):\n",
    "    \"\"\"Generate a test image to track training progress\"\"\"\n",
    "    \n",
    "    # Load models\n",
    "    base_model = \"black-forest-labs/FLUX.1-dev\"\n",
    "    \n",
    "    controlnet = FluxControlNetModel.from_pretrained(\n",
    "        controlnet_model_path, \n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    pipe = FluxControlNetPipeline.from_pretrained(\n",
    "        base_model, \n",
    "        controlnet=controlnet, \n",
    "        torch_dtype=torch.bfloat16\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Set resolution\n",
    "    width, height = 1024, 512\n",
    "    \n",
    "    # Set font\n",
    "    font_path = \"./arabic_fonts/Amiri-Regular.ttf\"\n",
    "    font_size = 80\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    \n",
    "    # Configure text\n",
    "    text_list = [test_text]\n",
    "    text_position_list = [(200, 200)]\n",
    "    text_color_list = [(255, 255, 255)]\n",
    "    \n",
    "    # Set controlnet conditions\n",
    "    control_image_list = []\n",
    "    control_position_list = []\n",
    "    control_mask_list = []\n",
    "    control_glyph_all = np.zeros([height, width, 3], dtype=np.uint8)\n",
    "    \n",
    "    # Handle each line of text\n",
    "    for text, text_position, text_color in zip(text_list, text_position_list, text_color_list):\n",
    "        # Glyph image\n",
    "        control_image_glyph = Image.new(\"RGB\", (width, height), (0, 0, 0))\n",
    "        draw = ImageDraw.Draw(control_image_glyph)\n",
    "        draw.text(text_position, text, font=font, fill=text_color)\n",
    "        \n",
    "        # Get bbox\n",
    "        bbox = draw.textbbox(text_position, text, font=font)\n",
    "        \n",
    "        # Position condition\n",
    "        control_position = np.zeros([height, width], dtype=np.uint8)\n",
    "        control_position[bbox[1]:bbox[3], bbox[0]:bbox[2]] = 255\n",
    "        control_position = Image.fromarray(control_position.astype(np.uint8))\n",
    "        control_position_list.append(control_position)\n",
    "        \n",
    "        # Regional mask\n",
    "        control_mask_np = np.zeros([height, width], dtype=np.uint8)\n",
    "        control_mask_np[bbox[1]-5:bbox[3]+5, bbox[0]-5:bbox[2]+5] = 255\n",
    "        control_mask = Image.fromarray(control_mask_np.astype(np.uint8))\n",
    "        control_mask_list.append(control_mask)\n",
    "        \n",
    "        # Accumulate glyph\n",
    "        control_glyph = np.array(control_image_glyph)\n",
    "        control_glyph_all += control_glyph\n",
    "        \n",
    "        # Canny condition\n",
    "        control_image = canny(cv2.cvtColor(np.array(control_image_glyph), cv2.COLOR_RGB2BGR))\n",
    "        control_image = Image.fromarray(cv2.cvtColor(control_image, cv2.COLOR_BGR2RGB))\n",
    "        control_image_list.append(control_image)\n",
    "    \n",
    "    control_glyph_all = Image.fromarray(control_glyph_all.astype(np.uint8))\n",
    "    control_glyph_all = control_glyph_all.convert(\"RGB\")\n",
    "    \n",
    "    # Set prompt\n",
    "    prompt = f\"a street sign in city, '{test_text}', filmfotos, film grain, reversal film photography\"\n",
    "    \n",
    "    # Generate\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        control_image=control_image_list,\n",
    "        control_position=control_position_list,\n",
    "        control_mask=control_mask_list,\n",
    "        control_glyph=control_glyph_all,\n",
    "        controlnet_conditioning_scale=1.0,\n",
    "        controlnet_conditioning_step=30,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=3.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    # Save\n",
    "    os.makedirs(\"./training_progress\", exist_ok=True)\n",
    "    output_path = f\"./training_progress/{epoch_name}.jpg\"\n",
    "    image.save(output_path)\n",
    "    \n",
    "    print(f\"‚úÖ Generated image: {output_path}\")\n",
    "    display(image)\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del pipe, controlnet\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Generate baseline image with original RepText\n",
    "print(\"Generating baseline image with original RepText model...\")\n",
    "baseline_image = generate_test_image(\"Shakker-Labs/RepText\", \"baseline_original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943455e",
   "metadata": {},
   "source": [
    "## Step 11: Generate Baseline Image (Before Training)\n",
    "\n",
    "Test the original RepText model before fine-tuning to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb92100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from controlnet_flux import FluxControlNetModel\n",
    "from pipeline_flux_controlnet import FluxControlNetPipeline\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load training config to match inference with training architecture\n",
    "config_path = \"./train_config.yaml\"\n",
    "if not Path(config_path).exists():\n",
    "    print(f\"‚ùå Error: {config_path} not found!\")\n",
    "    print(\"Make sure you've trained the model first.\")\n",
    "else:\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Get the trained model path\n",
    "    checkpoint_dir = config['output']['output_dir']\n",
    "    model_path = os.path.join(checkpoint_dir, \"final_model\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ùå Error: Trained model not found at {model_path}\")\n",
    "        print(f\"Available checkpoints in {checkpoint_dir}:\")\n",
    "        if os.path.exists(checkpoint_dir):\n",
    "            for item in os.listdir(checkpoint_dir):\n",
    "                print(f\"  - {item}\")\n",
    "    else:\n",
    "        print(f\"‚úì Loading trained ControlNet from {model_path}\")\n",
    "        \n",
    "        try:\n",
    "            base_model = config['model']['base_model']\n",
    "            \n",
    "            # Load ControlNet WITHOUT config overrides (use what's in checkpoint)\n",
    "            print(\"Loading ControlNet...\")\n",
    "            controlnet = FluxControlNetModel.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.bfloat16\n",
    "            )\n",
    "            \n",
    "            # Load inference pipeline\n",
    "            print(\"Loading FLUX pipeline...\")\n",
    "            pipe = FluxControlNetPipeline.from_pretrained(\n",
    "                base_model,\n",
    "                controlnet=controlnet,\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            print(\"‚úÖ Models loaded successfully!\")\n",
    "            print(f\"   Base Model: {base_model}\")\n",
    "            print(f\"   ControlNet In Channels: {controlnet.config.in_channels}\")\n",
    "            print(f\"   ControlNet X-Embedder In Features: {controlnet.x_embedder.in_features}\")\n",
    "            print(f\"   ControlNet Layers: {controlnet.config.num_layers}\")\n",
    "            print(f\"   ControlNet Single Layers: {controlnet.config.num_single_layers}\")\n",
    "            \n",
    "            # Check if x_embedder matches in_channels (both in packed format)\n",
    "            print(f\"\\nüìã Architecture Check:\")\n",
    "            print(f\"   Config in_channels: {controlnet.config.in_channels}\")\n",
    "            print(f\"   X-Embedder input features: {controlnet.x_embedder.in_features}\")\n",
    "            print(f\"   Note: in_channels = VAE channels (16) √ó 4 from 2√ó2 spatial packing\")\n",
    "            \n",
    "            # Check compatibility\n",
    "            if controlnet.x_embedder.in_features == controlnet.config.in_channels:\n",
    "                print(f\"\\n‚úÖ COMPATIBLE - Ready for inference!\")\n",
    "                print(f\"   Both use the same {controlnet.config.in_channels}-dimensional packed format.\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  WARNING: Possible mismatch detected!\")\n",
    "                print(f\"   X-Embedder expects: {controlnet.x_embedder.in_features} features\")\n",
    "                print(f\"   Config specifies: {controlnet.config.in_channels} channels\")\n",
    "                print(f\"   This might cause errors during inference.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading models: {e}\")\n",
    "            print(\"\\nMake sure:\")\n",
    "            print(\"  1. Training has completed\")\n",
    "            print(\"  2. The model checkpoint exists\")\n",
    "            print(\"  3. Your GPU has enough memory (24GB+)\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Compatibility Check\n",
    "if 'controlnet' in globals():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"QUICK COMPATIBILITY CHECK\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    x_embedder_in = controlnet.x_embedder.in_features\n",
    "    config_in_channels = controlnet.config.in_channels\n",
    "    \n",
    "    print(f\"\\nControlNet Configuration:\")\n",
    "    print(f\"  ‚Ä¢ X-Embedder input features: {x_embedder_in}\")\n",
    "    print(f\"  ‚Ä¢ Config in_channels: {config_in_channels}\")\n",
    "    print(f\"  ‚Ä¢ (in_channels = 16 VAE channels √ó 4 from 2√ó2 spatial packing)\")\n",
    "    \n",
    "    if x_embedder_in == config_in_channels:\n",
    "        print(f\"\\n‚úÖ COMPATIBLE - Inference should work perfectly!\")\n",
    "        print(f\"   Both training and inference use the same {config_in_channels}-dimensional\")\n",
    "        print(f\"   packed latent format with 2√ó2 spatial packing.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå INCOMPATIBLE - Dimension mismatch detected!\")\n",
    "        print(f\"   X-Embedder expects: {x_embedder_in}\")\n",
    "        print(f\"   Config provides: {config_in_channels}\")\n",
    "        print(f\"\\n   SOLUTION: Retrain with correct config\")\n",
    "        print(f\"     accelerate launch train_arabic.py --config train_config.yaml\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"ControlNet not loaded. Run model loading cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9158329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_glyph_and_controls(text, font_path=\"./arabic_fonts/Amiri-Regular.ttf\", \n",
    "                               font_size=80, width=1024, height=1024):\n",
    "    \"\"\"\n",
    "    Prepare glyph, canny, and position maps for inference.\n",
    "    \n",
    "    Args:\n",
    "        text: Arabic text string\n",
    "        font_path: Path to Arabic font\n",
    "        font_size: Font size for rendering\n",
    "        width: Image width\n",
    "        height: Image height\n",
    "    \n",
    "    Returns:\n",
    "        glyph: Rendered text image (RGB)\n",
    "        canny: Canny edge detection of glyph (RGB)\n",
    "        position: Position heatmap (grayscale for pipeline compatibility)\n",
    "    \"\"\"\n",
    "    # Render text (glyph)\n",
    "    glyph_img = Image.new('RGB', (width, height), color='white')\n",
    "    draw = ImageDraw.Draw(glyph_img)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load font {font_path}: {e}\")\n",
    "        print(\"Using default font instead\")\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Draw text\n",
    "    draw.text((50, (height - font_size) // 2), text, font=font, fill='black')\n",
    "    \n",
    "    # Create canny edges\n",
    "    glyph_array = np.array(glyph_img.convert('L'))\n",
    "    canny_edges = cv2.Canny(glyph_array, 50, 100)\n",
    "    canny_img = Image.fromarray(np.dstack([255 - canny_edges, 255 - canny_edges, 255 - canny_edges]))\n",
    "    \n",
    "    # Create position map as GRAYSCALE (single channel)\n",
    "    # The pipeline will expand it to 3 channels automatically\n",
    "    position_array = np.zeros((height, width), dtype=np.uint8)\n",
    "    position_array[100:height-100, 50:width-50] = 200\n",
    "    position_img = Image.fromarray(position_array)  # Grayscale image\n",
    "    \n",
    "    return glyph_img, canny_img, position_img\n",
    "\n",
    "def generate_image(text, prompt=\"\", num_inference_steps=50, controlnet_conditioning_step=30, \n",
    "                   output_path=\"./results\"):\n",
    "    \"\"\"\n",
    "    Generate an image with Arabic text using the trained ControlNet.\n",
    "    \n",
    "    Args:\n",
    "        text: Arabic text to render\n",
    "        prompt: Optional prompt for FLUX (default: empty for unconditional generation)\n",
    "        num_inference_steps: Number of inference steps (more = higher quality but slower)\n",
    "        controlnet_conditioning_step: When to stop applying ControlNet (0-num_inference_steps)\n",
    "        output_path: Directory to save output image\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'pipe' not in globals():\n",
    "        print(\"‚ùå Pipeline not loaded. Please run the model loading cell first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Generating image for: {text}\")\n",
    "        \n",
    "        # Prepare conditioning\n",
    "        glyph, canny, position = prepare_glyph_and_controls(text)\n",
    "        \n",
    "        # Resize to match model input\n",
    "        glyph = glyph.resize((512, 512))\n",
    "        position = position.resize((512, 512))\n",
    "        \n",
    "        # Use empty prompt for unconditional generation\n",
    "        if not prompt:\n",
    "            prompt = \"\"\n",
    "        \n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print(f\"Inference steps: {num_inference_steps}\")\n",
    "        print(f\"ControlNet conditioning until step: {controlnet_conditioning_step}\")\n",
    "        \n",
    "        # Generate image with glyph as the control anchor and position as spatial guide\n",
    "        # control_glyph: used to initialize latents from rendered text\n",
    "        # control_image: primary spatial control (glyph)\n",
    "        # control_position: position guidance (passed as grayscale, expanded to 3ch by pipeline)\n",
    "        with torch.no_grad():\n",
    "            generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "            \n",
    "            image = pipe(\n",
    "                prompt,\n",
    "                height=512,\n",
    "                width=512,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=0.0,  # No guidance for unconditional\n",
    "                controlnet_conditioning_scale=1.0,\n",
    "                controlnet_conditioning_step=controlnet_conditioning_step,\n",
    "                control_image=[glyph],  # RGB glyph as main control\n",
    "                control_position=[position],  # Grayscale position map (expanded to 3ch by pipeline)\n",
    "                control_glyph=glyph,  # used for latent initialization\n",
    "                control_mask=None,\n",
    "                generator=generator,\n",
    "            ).images[0]\n",
    "        \n",
    "        # Save image\n",
    "        output_file = os.path.join(output_path, f\"generated_{text[:10]}.png\")\n",
    "        image.save(output_file)\n",
    "        print(f\"‚úÖ Image saved to {output_file}\")\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during generation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test generation\n",
    "print(\"Ready to generate images with Arabic text!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print('  image = generate_image(\"ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ\")')\n",
    "print('  image = generate_image(\"ŸÖÿ±ÿ≠ÿ®ÿß\", prompt=\"A beautiful greeting\", num_inference_steps=50)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1815aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate Arabic text images\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE EXAMPLE - Generate Arabic Text Images\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if models are loaded\n",
    "if 'pipe' not in globals() or 'controlnet' not in globals():\n",
    "    print(\"‚ùå ERROR: Models not loaded!\")\n",
    "    print(\"Please run the model loading cell (Step 11) first.\")\n",
    "else:\n",
    "    # Try generating images\n",
    "    test_texts = [\n",
    "        \"ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ\",  # Hello/Peace be upon you\n",
    "        \"ŸÖÿ±ÿ≠ÿ®ÿß\",           # Hello\n",
    "        \"ÿ¥ŸÉÿ±ÿß\",            # Thank you\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for text in test_texts:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        try:\n",
    "            image = generate_image(\n",
    "                text=text,\n",
    "                prompt=\"\",  # Empty prompt for unconditional generation\n",
    "                num_inference_steps=30,  # Reduced for faster testing\n",
    "                controlnet_conditioning_step=20,\n",
    "                output_path=\"./results_after_training\"\n",
    "            )\n",
    "            if image:\n",
    "                results.append((text, image))\n",
    "                # Display the image\n",
    "                from IPython.display import display\n",
    "                display(image)\n",
    "        except RuntimeError as e:\n",
    "            error_msg = str(e)\n",
    "            if \"shapes cannot be multiplied\" in error_msg:\n",
    "                print(f\"‚ùå Architecture Mismatch Error: {e}\")\n",
    "                print(\"\\nThis error occurs when the checkpoint architecture doesn't match\")\n",
    "                print(\"what the pipeline expects. This typically means:\")\n",
    "                print(\"  1. The checkpoint was trained with different settings\")\n",
    "                print(\"  2. The in_channels or packing configuration doesn't match\")\n",
    "                print(\"\\n‚Üí CHECK THE DIAGNOSTIC CELL ABOVE to see the mismatch\")\n",
    "                print(\"‚Üí YOU MAY NEED TO RETRAIN THE MODEL with the current config\")\n",
    "            else:\n",
    "                print(f\"‚ùå Error during generation: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ Generated {len(results)}/{len(test_texts)} images successfully!\")\n",
    "    \n",
    "    if len(results) < len(test_texts):\n",
    "        print(\"\\n‚ö†Ô∏è  Some generations failed. See errors above for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b0733",
   "metadata": {},
   "source": [
    "## Troubleshooting Inference\n",
    "\n",
    "### Current Status: Training in Progress ‚úÖ\n",
    "\n",
    "**What's happening on RunPod:**\n",
    "```bash\n",
    "Loaded 90 samples for train split\n",
    "Loaded 10 samples for val split\n",
    "# Training running with in_channels: 64 (packed format)\n",
    "```\n",
    "\n",
    "Your training is now running with the **correct configuration** (`in_channels: 64`). This means the checkpoint being saved will be **100% compatible with inference**.\n",
    "\n",
    "### Why the Old Checkpoint Failed\n",
    "\n",
    "The old checkpoint was trained with `in_channels: 16`, but inference provides 64-dimensional packed latents:\n",
    "- **Old checkpoint**: `x_embedder` configured for 16 features\n",
    "- **Inference pipeline**: Provides 64-dimensional packed latents (VAE 16 channels √ó 4 from 2√ó2 spatial packing)\n",
    "- **Result**: Shape mismatch error `1024x128 and 64x3072`\n",
    "\n",
    "### What to Watch During Training\n",
    "\n",
    "**Expected Training Metrics:**\n",
    "```\n",
    "Loss: Should gradually decrease from 0-1.0 range\n",
    "Learning rate: Starts low during warmup, increases to 1e-5 by step 500\n",
    "Epoch time: ~1-2 min per epoch depending on GPU\n",
    "```\n",
    "\n",
    "**Example of healthy progress:**\n",
    "- Epoch 0-10: Loss gradually decreasing\n",
    "- Epoch 10-50: Loss continuing to decrease, may stabilize\n",
    "- Epoch 50+: Loss should be near minimum (~0.01 or lower)\n",
    "\n",
    "### What Happens After Training Completes ‚úÖ\n",
    "\n",
    "Once training finishes, you'll have:\n",
    "1. ‚úÖ `output/arabic_reptext/final_model/` - The trained checkpoint\n",
    "2. ‚úÖ Checkpoint built with `in_channels: 64` (perfect match for inference)\n",
    "3. ‚úÖ Ready for inference - no more architecture mismatches!\n",
    "\n",
    "**Then run inference in the notebook:**\n",
    "```python\n",
    "# Run the model loading cell ‚Üí it will load final_model\n",
    "# Run the inference cell ‚Üí should work without errors!\n",
    "image = generate_image(\"ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ\")\n",
    "```\n",
    "\n",
    "### Critical Issue (RESOLVED): \"Cannot multiply shapes (1024x64 and 16x3072)\"\n",
    "\n",
    "**Status: FIXED** ‚úÖ\n",
    "\n",
    "**What was wrong:**\n",
    "- Your checkpoint was trained with `in_channels: 16`\n",
    "- The inference pipeline always provides 64-dimensional packed latents\n",
    "- Dimension mismatch caused the error\n",
    "\n",
    "**What you did to fix it:**\n",
    "1. Deleted old checkpoint: `rm -rf output/arabic_reptext/`\n",
    "2. Started fresh training with current config: `in_channels: 64`\n",
    "3. New checkpoint will be compatible with inference\n",
    "\n",
    "**You won't see this error again** because you're now training with the correct settings.\n",
    "\n",
    "### Other Common Issues\n",
    "\n",
    "**Issue: Out of memory during inference**\n",
    "- Solution: Reduce `num_inference_steps` (try 20-30 instead of 50)\n",
    "- Or reduce image size in `prepare_glyph_and_controls`\n",
    "\n",
    "**Issue: Poor quality results**\n",
    "- Solution: Increase `num_inference_steps` (try 50-100)\n",
    "- Make sure training completed with loss converging to low values\n",
    "\n",
    "**Issue: \"Model not found\" error**\n",
    "- Make sure training completed: check `./output/arabic_reptext/final_model/`\n",
    "- Or check for intermediate checkpoints: `./output/arabic_reptext/checkpoint-XXXX/`\n",
    "\n",
    "### Debug Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a848878",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checkpoint Architecture Diagnostics\n",
    "\n",
    "Use this cell to verify what the loaded ControlNet expects vs what the pipeline provides.\n",
    "\n",
    "**If you see a mismatch warning, you likely need to retrain the model** with the current `train_config.yaml` to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ControlNet Architecture\n",
    "if 'controlnet' in globals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ControlNet Architecture\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Input Channels (in_channels): {controlnet.config.in_channels}\")\n",
    "    print(f\"Output Channels (out_channels): {controlnet.out_channels}\")\n",
    "    print(f\"Inner Dim: {controlnet.inner_dim}\")\n",
    "    print(f\"Transformer Blocks: {controlnet.config.num_layers}\")\n",
    "    print(f\"Single Transformer Blocks: {controlnet.config.num_single_layers}\")\n",
    "    \n",
    "    # Check x_embedder dimensions\n",
    "    print(f\"\\nX-Embedder Layer (input embedder):\")\n",
    "    print(f\"  Input Features: {controlnet.x_embedder.in_features}\")\n",
    "    print(f\"  Output Features: {controlnet.x_embedder.out_features}\")\n",
    "    print(f\"\\nContext Embedder Layer:\")\n",
    "    print(f\"  Input Features: {controlnet.context_embedder.in_features}\")\n",
    "    print(f\"  Output Features: {controlnet.context_embedder.out_features}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ùå ControlNet not loaded yet. Run the model loading cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2469538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug Information\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE DEBUG INFO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check config\n",
    "print(\"\\n1. Training Config:\")\n",
    "if Path(\"train_config.yaml\").exists():\n",
    "    with open(\"train_config.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"   ‚úì Config found\")\n",
    "    print(f\"   - Image size: {config['data']['image_size']}\")\n",
    "    print(f\"   - Batch size: {config['data']['batch_size']}\")\n",
    "    print(f\"   - ControlNet layers: {config['model']['controlnet_config']['num_layers']}\")\n",
    "    print(f\"   - ControlNet single layers: {config['model']['controlnet_config']['num_single_layers']}\")\n",
    "    print(f\"   - Text seq len: {config['model'].get('text_seq_len', 'Not set')}\")\n",
    "else:\n",
    "    print(f\"   ‚úó Config not found\")\n",
    "\n",
    "# Check model checkpoint\n",
    "print(\"\\n2. Model Checkpoint:\")\n",
    "output_dir = config['output']['output_dir'] if 'config' in locals() else \"./output/arabic_reptext\"\n",
    "if os.path.exists(output_dir):\n",
    "    items = os.listdir(output_dir)\n",
    "    print(f\"   ‚úì Output directory found: {output_dir}\")\n",
    "    print(f\"   - Contents: {items}\")\n",
    "    \n",
    "    final_model = os.path.join(output_dir, \"final_model\")\n",
    "    if os.path.exists(final_model):\n",
    "        print(f\"   ‚úì Final model found: {final_model}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö† Final model not found (training may still be in progress)\")\n",
    "else:\n",
    "    print(f\"   ‚úó Output directory not found: {output_dir}\")\n",
    "\n",
    "# Check GPU\n",
    "print(\"\\n3. GPU Status:\")\n",
    "print(f\"   - CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   - GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"   - Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   - Memory used: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "\n",
    "# Check fonts\n",
    "print(\"\\n4. Arabic Fonts:\")\n",
    "if os.path.exists(\"./arabic_fonts\"):\n",
    "    fonts = [f for f in os.listdir(\"./arabic_fonts\") if f.endswith(('.ttf', '.otf'))]\n",
    "    print(f\"   ‚úì Fonts directory found\")\n",
    "    print(f\"   - Number of fonts: {len(fonts)}\")\n",
    "    if fonts:\n",
    "        print(f\"   - Sample: {fonts[0]}\")\n",
    "else:\n",
    "    print(f\"   ‚úó Fonts directory not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39d01f",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Completed ‚úÖ\n",
    "- ‚úÖ Installed dependencies\n",
    "- ‚úÖ Downloaded Arabic fonts\n",
    "- ‚úÖ Prepared training dataset\n",
    "- ‚úÖ Configured training with proper ControlNet architecture\n",
    "- ‚úÖ Set up inference with matching model config\n",
    "- ‚úÖ Created simplified inference scripts\n",
    "\n",
    "### What You've Learned\n",
    "1. How to prepare Arabic text training data\n",
    "2. How to configure and train ControlNet with FLUX\n",
    "3. How to handle GPU memory constraints (48GB setup)\n",
    "4. How to run inference with the trained model\n",
    "\n",
    "### To TRAIN the Model\n",
    "Run in terminal:\n",
    "```bash\n",
    "# Single GPU\n",
    "accelerate launch train_arabic.py --config train_config.yaml\n",
    "\n",
    "# Dual GPU (Recommended for 2x48GB)\n",
    "accelerate config  # Set Number of processes: 2\n",
    "accelerate launch --num_processes 2 train_arabic.py --config train_config.yaml\n",
    "```\n",
    "\n",
    "### To RUN INFERENCE\n",
    "Option 1 - Use Notebook (Already configured):\n",
    "```python\n",
    "# Run the cells above to load model and generate images\n",
    "image = generate_image(\"ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ\")\n",
    "```\n",
    "\n",
    "Option 2 - Use Terminal Script:\n",
    "```bash\n",
    "python infer_simple.py --text \"ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ\" --num_steps 50\n",
    "```\n",
    "\n",
    "### Key Files\n",
    "- `train_config.yaml` - Training configuration (single source of truth)\n",
    "- `train_arabic.py` - Training script\n",
    "- `infer_simple.py` - Simplified inference script\n",
    "- `arabic_training_quickstart.ipynb` - This notebook\n",
    "\n",
    "### Tips for Best Results\n",
    "1. **Training**: Use 2 GPUs for faster training\n",
    "2. **Inference**: Increase `num_steps` (50-100) for higher quality\n",
    "3. **Memory**: If OOM, reduce `image_size` or `batch_size` in config\n",
    "4. **Fonts**: Use diverse Arabic fonts for better generalization\n",
    "5. **Dataset**: More training samples = better results\n",
    "\n",
    "### Resources\n",
    "- [RepText Paper](https://arxiv.org/abs/2504.19724)\n",
    "- [TRAINING_GUIDE.md](TRAINING_GUIDE.md) - Detailed documentation\n",
    "- [TRAINING_CONFIG_GUIDE.md](TRAINING_CONFIG_GUIDE.md) - Configuration guide"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
