# RepText Arabic Training Configuration (Single Source of Truth)

# Data
data:
  data_dir: "./arabic_training_data"
  image_size: [512, 512]
  train_ratio: 0.9
  batch_size: 1
  num_workers: 4

# Model
model:
  base_model: "black-forest-labs/FLUX.1-dev"
  pretrained_controlnet: "Shakker-Labs/RepText"
  text_seq_len: 128
  controlnet_config:
    in_channels: 64
    extra_condition_channels: 64
    num_layers: 2
    num_single_layers: 4
    attention_head_dim: 128
    num_attention_heads: 24
    
# Training
training:
  num_epochs: 10
  learning_rate: 1.0e-5
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_weight_decay: 1.0e-2
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0
  
  # Loss weights
  diffusion_loss_weight: 1.0
  text_perceptual_loss_weight: 0.0
  use_reconstruction_loss: false
  
  # Learning rate schedule
  lr_scheduler: "constant_with_warmup"
  lr_warmup_steps: 500
  
  # Mixed precision
  mixed_precision: "bf16"
  
  # Gradient accumulation
  gradient_accumulation_steps: 4
  
  # Checkpointing
  save_steps: 1000
  eval_steps: 500
  logging_steps: 50
  
# Output
output:
  output_dir: "./output/arabic_reptext"
  logging_dir: "./output/arabic_reptext/logs"
  
# OCR Model for Text Perceptual Loss
ocr:
  model_name: "microsoft/trocr-base-printed"
  use_ocr_loss: false

